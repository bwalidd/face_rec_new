# Multi-GPU Backend Configuration - 2 GPUs per Pod
# GPU0: 2x RTX 2080 Ti on Master | GPU1: 2x RTX Super on Worker

# Django Backend GPU0 - MASTER NODE (2x RTX 2080 Ti)
apiVersion: apps/v1
kind: Deployment
metadata:
  name: django-backend-gpu0
  namespace: face-recognition-system
  labels:
    app: django-backend
    gpu-id: "gpu0"
    component: backend
    gpu-count: "2"
    gpu-type: "ti"
    build: "multi-gpu"
spec:
  replicas: 1
  selector:
    matchLabels:
      app: django-backend
      gpu-id: "gpu0"
      build: "multi-gpu"
  template:
    metadata:
      labels:
        app: django-backend
        gpu-id: "gpu0"
        component: backend
        node-type: "master"
        gpu-count: "2"
        gpu-type: "ti"
        build: "multi-gpu"
    spec:
      nodeSelector:
        kubernetes.io/hostname: bluedove2-ms-7b98
      containers:
      - name: django-backend
        image: bwalidd/new-django:base-image-fix1
        workingDir: /app/Backend
        command: ["python", "manage.py", "runserver", "0.0.0.0:9898"]
        ports:
        - containerPort: 9898
        - containerPort: 6006
        - containerPort: 8888
        env:
        - name: CUDA_VISIBLE_DEVICES
          value: "0,1"
        - name: NVIDIA_VISIBLE_DEVICES
          value: "0,1"
        - name: GPU_ID
          value: "0,1"
        - name: GPU_COUNT
          value: "2"
        - name: NODE_TYPE
          value: "master"
        - name: BACKEND_INSTANCE
          value: "gpu0-multi"
        - name: GPU_TYPE
          value: "RTX 2080 Ti"
        - name: MULTI_GPU_MODE
          value: "true"
        - name: PYTORCH_CUDA_ALLOC_CONF
          value: "max_split_size_mb:256"
        - name: NCCL_DEBUG
          value: "INFO"
        - name: NCCL_IB_DISABLE
          value: "1"
        - name: NCCL_P2P_DISABLE
          value: "1"
        - name: CUDA_VERSION
          value: "12.2"
        - name: CUDA_LAUNCH_BLOCKING
          value: "0"
        - name: OMP_NUM_THREADS
          value: "4"
        - name: MALLOC_TRIM_THRESHOLD_
          value: "100000"
        - name: STREAM_URL
          valueFrom:
            configMapKeyRef:
              name: app-config
              key: STREAM_URL
        resources:
          limits:
            nvidia.com/gpu: 2
            cpu: "2000m"
            memory: "4Gi"
          requests:
            nvidia.com/gpu: 2
            cpu: "1000m"
            memory: "2Gi"

---
# Django Backend GPU1 - WORKER NODE (2x RTX Super)
apiVersion: apps/v1
kind: Deployment
metadata:
  name: django-backend-gpu1
  namespace: face-recognition-system
  labels:
    app: django-backend
    gpu-id: "gpu1"
    component: backend
    gpu-count: "2"
    gpu-type: "super"
    build: "multi-gpu"
spec:
  replicas: 1
  selector:
    matchLabels:
      app: django-backend
      gpu-id: "gpu1"
      build: "multi-gpu"
  template:
    metadata:
      labels:
        app: django-backend
        gpu-id: "gpu1"
        component: backend
        node-type: "worker"
        gpu-count: "2"
        gpu-type: "super"
        build: "multi-gpu"
    spec:
      nodeSelector:
        kubernetes.io/hostname: bluedovve-ms-7b98
      affinity:
        podAntiAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
          - labelSelector:
              matchLabels:
                gpu-id: "gpu0"
            topologyKey: kubernetes.io/hostname
      containers:
      - name: django-backend
        image: bwalidd/new-django:cuda-12.4
        workingDir: /app/Backend
        command: ["python", "manage.py", "runserver", "0.0.0.0:9898"]
        ports:
        - containerPort: 9898
        - containerPort: 6006
        - containerPort: 8888
        env:
        - name: CUDA_VISIBLE_DEVICES
          value: "0,1"
        - name: NVIDIA_VISIBLE_DEVICES
          value: "0,1"
        - name: GPU_ID
          value: "0,1"
        - name: GPU_COUNT
          value: "2"
        - name: NODE_TYPE
          value: "worker"
        - name: BACKEND_INSTANCE
          value: "gpu1-multi"
        - name: GPU_TYPE
          value: "RTX Super"
        - name: MULTI_GPU_MODE
          value: "true"
        - name: PYTORCH_CUDA_ALLOC_CONF
          value: "max_split_size_mb:256"
        - name: NCCL_DEBUG
          value: "INFO"
        - name: NCCL_IB_DISABLE
          value: "1"
        - name: NCCL_P2P_DISABLE
          value: "1"
        - name: CUDA_VERSION
          value: "12.4"
        - name: CUDA_LAUNCH_BLOCKING
          value: "0"
        - name: OMP_NUM_THREADS
          value: "4"
        - name: MALLOC_TRIM_THRESHOLD_
          value: "100000"
        - name: STREAM_URL
          valueFrom:
            configMapKeyRef:
              name: app-config
              key: STREAM_URL
        resources:
          limits:
            nvidia.com/gpu: 2
            cpu: "2000m"
            memory: "4Gi"
          requests:
            nvidia.com/gpu: 2
            cpu: "1000m"
            memory: "2Gi"
        
